#!/usr/bin/env python
# -*- coding: utf-8 -*-
"""
@Time   : 10/21/25
@Author : guojarrett@gmail.com
@File   : processor.py
"""

import time
from typing import TYPE_CHECKING

from src.utils.logger import logger

if TYPE_CHECKING:
    from src.core.assistant import VoiceAssistant


class CommandProcessor:
    """å‘½ä»¤å¤„ç†å™¨ - è´Ÿè´£å¤„ç†ç”¨æˆ·æŒ‡ä»¤"""

    def __init__(self, assistant: 'VoiceAssistant'):
        self.assistant = assistant
        self.config = assistant.config

    def process_command(self):
        """å¤„ç†ç”¨æˆ·æŒ‡ä»¤çš„ä¸»æµç¨‹"""
        self.assistant.is_processing = True

        try:
            # æš‚åœå”¤é†’è¯æ£€æµ‹,é¿å…éº¦å…‹é£å†²çª
            logger.info("â¸ï¸  Pausing wake word detection...")
            self.assistant.detector.pause()

            # ç­‰å¾…ä¸€å°æ®µæ—¶é—´,ç¡®ä¿éº¦å…‹é£é‡Šæ”¾
            time.sleep(0.2)

            # 1. å½•éŸ³
            audio_data = self._record_audio()
            if audio_data is None:
                logger.warning("âš ï¸  å½•éŸ³è¢«å–æ¶ˆæˆ–æ—¶é•¿ä¸è¶³ï¼Œè·³è¿‡å¤„ç†")
                return

            # 2. è¯­éŸ³è¯†åˆ« (ASR)
            text = self._transcribe_audio(audio_data)
            if not text:
                return

            logger.info(f"ğŸ“ Recognized text: {text}")

            # 3. ç†è§£æ„å›¾å¹¶è§„åˆ’ä»»åŠ¡
            plan = self._understand_and_plan(text)

            # 4. æ‰§è¡Œä»»åŠ¡
            result = self._execute_plan(plan)

            # 5. è¯­éŸ³åé¦ˆ
            self._text_to_speech(result)

            logger.info("âœ… Processing completed")

        except Exception as e:
            logger.error(f"âŒ Processing failed: {e}")
            import traceback
            traceback.print_exc()

        finally:
            self.assistant.is_processing = False

            # ç­‰å¾…å½•éŸ³å™¨å®Œå…¨é‡Šæ”¾èµ„æº
            time.sleep(0.3)

            # æ¢å¤å”¤é†’è¯æ£€æµ‹
            logger.info("â–¶ï¸  Resuming wake word detection...")
            self.assistant.detector.resume()

            logger.info("ğŸ¤ Listening for wake words...\n")

    def _record_audio(self) -> bytes:
        """å½•åˆ¶éŸ³é¢‘ï¼ˆæ”¯æŒåŠ¨æ€æ—¶é•¿ï¼‰"""
        logger.info("ğŸ™ï¸  Please speak your command...")
        min_duration = self.config.get("recording.dynamic.min_duration", 2.0)
        max_duration = self.config.get("recording.dynamic.max_duration", 60.0)
        silence_threshold = self.config.get("recording.dynamic.silence_threshold", 500.0)
        silence_duration = self.config.get("recording.dynamic.silence_duration", 3.0)
        speech_threshold = self.config.get("recording.dynamic.speech_threshold", 800.0)
        min_speech_chunks = self.config.get("recording.dynamic.min_speech_chunks", 5)

        audio_data = self.assistant.recorder.record_with_silence_detection(
            min_duration=min_duration,
            max_duration=max_duration,
            silence_threshold=silence_threshold,
            silence_duration=silence_duration,
            speech_threshold=speech_threshold,
            min_speech_chunks=min_speech_chunks
        )

        return audio_data

    def _transcribe_audio(self, audio_data: bytes) -> str:
        """è¯­éŸ³è¯†åˆ«"""
        logger.info("ğŸ”„ Converting speech to text...")

        # æ£€æŸ¥éŸ³é¢‘èƒ½é‡,è¿‡æ»¤æ‰çº¯é™éŸ³
        if not self._has_valid_speech(audio_data):
            logger.warning("âš ï¸  Audio contains only silence or noise, skipping transcription")
            return ""

        if self.assistant.asr_provider == "whisper":
            # æœ¬åœ° Whisper
            result = self.assistant.asr_client.transcribe_from_bytes(
                audio_data=audio_data,
                audio_format="wav",
                language=self.assistant.asr_language
            )
        else:
            # ä¸ƒç‰›äº‘ (éœ€è¦ URL,è¿™é‡Œä¼šå¤±è´¥)
            logger.error("âŒ Qiniu ASR requires URL, not supported for realtime")
            return ""

        text = result.get("text", "").strip()

        if not text:
            logger.warning("âš ï¸  No speech detected or recognized")
            return ""

        # è¿‡æ»¤ Whisper çš„å¸¸è§å¹»è§‰è¾“å‡º
        if self._is_hallucination(text):
            logger.warning(f"âš ï¸  Detected hallucination output: '{text}', ignoring")
            return ""

        return text

    def _has_valid_speech(self, audio_data: bytes, threshold: float = 100.0) -> bool:
        """æ£€æŸ¥éŸ³é¢‘æ˜¯å¦åŒ…å«æœ‰æ•ˆè¯­éŸ³(åŸºäºRMSèƒ½é‡)"""
        import numpy as np
        import wave
        import io

        try:
            # è¯»å– WAV æ•°æ®
            with wave.open(io.BytesIO(audio_data), 'rb') as wf:
                frames = wf.readframes(wf.getnframes())
                audio_array = np.frombuffer(frames, dtype=np.int16)

            # è®¡ç®—æ•´ä½“ RMS
            rms = np.sqrt(np.mean(audio_array.astype(np.float64) ** 2))

            logger.debug(f"   Audio RMS: {rms:.1f}, threshold: {threshold}")

            return rms > threshold

        except Exception as e:
            logger.warning(f"âš ï¸  Error checking audio energy: {e}")
            return True  # å‡ºé”™æ—¶å…è®¸ç»§ç»­è¯†åˆ«

    @classmethod
    def _is_hallucination(cls, text: str) -> bool:
        """æ£€æµ‹æ˜¯å¦ä¸º Whisper çš„å¹»è§‰è¾“å‡º"""
        # å¸¸è§çš„ Whisper å¹»è§‰æ¨¡å¼
        hallucination_patterns = [
            "å­—å¹•",
            "ç¿»è¯‘",
            "è°¢è°¢è§‚çœ‹",
            "è¯·ä¸åç‚¹èµ",
            "è®¢é˜…",
            "å…³æ³¨",
            "by",
            "æ„Ÿè°¢",
            "æˆ‘ä»¬ä¸‹æœŸå†è§",
            "æ‹œæ‹œ",
            "ç´¢å…°å¨…",
            "subtitle",
            "amara",
            "å­—å¹•ç»„",
            # å¯ä»¥æ ¹æ®å®é™…æƒ…å†µæ·»åŠ æ›´å¤š
        ]

        text_lower = text.lower()

        # æ£€æŸ¥æ˜¯å¦åŒ…å«å¹»è§‰å…³é”®è¯
        for pattern in hallucination_patterns:
            if pattern in text_lower:
                return True

        # æ£€æŸ¥é•¿åº¦(çœŸå®è¯­éŸ³é€šå¸¸ä¸ä¼šå¤ªçŸ­)
        if len(text.strip()) < 2:
            return True

        return False

    def _understand_and_plan(self, text: str) -> dict:
        """ç†è§£æ„å›¾å¹¶ç”Ÿæˆæ‰§è¡Œè®¡åˆ’"""
        # TODO: è°ƒç”¨ LLM è¿›è¡Œæ„å›¾ç†è§£å’Œä»»åŠ¡è§„åˆ’
        logger.info("ğŸ§  Understanding intent and planning...")
        logger.info(f"   User said: {text}")

        return {
            "intent": "unknown",
            "text": text,
            "actions": []
        }

    def _execute_plan(self, plan: dict) -> str:
        """æ‰§è¡Œä»»åŠ¡è®¡åˆ’"""
        # TODO: è°ƒç”¨å·¥å…·æ‰§è¡Œä»»åŠ¡
        logger.info("âš™ï¸  Executing plan...")
        text = plan.get("text", "")
        return f"Received your command: {text}"

    def _text_to_speech(self, text: str):
        """æ–‡å­—è½¬è¯­éŸ³"""
        # TODO: è°ƒç”¨ TTS API
        logger.info("ğŸ”Š Providing voice feedback...")
        logger.info(f"ğŸ’¬ Response: {text}")
