#!/usr/bin/env python
# -*- coding: utf-8 -*-
"""
@Time   : 10/25/25
@Author : guojarrett@gmail.com
@File   : LLMFactory.py
"""

from typing import Dict

from langchain_community.chat_models import ChatOllama
from langchain_openai import ChatOpenAI

from src.utils.config import config
from src.utils.logger import logger


class LLMFactory:
    """
    LLM å·¥å‚ç±» - æ ¹æ®ç”¨é€”åˆ›å»ºä¸åŒçš„æ¨¡å‹

    åˆ†çº§ç­–ç•¥ï¼š
    - Planner:  qwen3-max (æœ€å¼ºæ¨ç†)
    - Worker:   qwen3-max-preview (å¿«é€Ÿå“åº”)
    - Summary:  qwen3-max-preview (å¿«é€Ÿæ€»ç»“)
    """

    _instances: Dict[str, ChatOpenAI] = {}

    @classmethod
    def get_llm(cls, llm_type: str = "worker") -> ChatOpenAI:
        """
        è·å– LLM å®ä¾‹ï¼ˆå•ä¾‹æ¨¡å¼ï¼‰

        Args:
            llm_type: planner | worker | summary

        Returns:
            ChatOpenAI å®ä¾‹
        """
        # æ£€æŸ¥ç¼“å­˜
        if llm_type in cls._instances:
            logger.debug(f"Using cached {llm_type} LLM")
            return cls._instances[llm_type]

        # è·å–åŸºç¡€é…ç½®
        qiniu_config = config.get("qiniu")
        if not qiniu_config:
            raise ValueError("Qiniu config not found")

        # è·å–æ¨¡å‹ç‰¹å®šé…ç½®
        model_config = config.get(f"qiniu.models.{llm_type}")

        if not model_config:
            logger.warning(
                f"No specific config for {llm_type}, using default worker config"
            )
            model_config = {
                "model": "qwen3-max-preview",
                "temperature": 0.0,
                "max_tokens": 500
            }

        # åˆ›å»ºå®ä¾‹
        llm = ChatOpenAI(
            api_key=qiniu_config.get("api_key"),
            base_url=qiniu_config.get("base_url"),
            model=model_config.get("model"),
            temperature=model_config.get("temperature", 0.0),
            max_tokens=model_config.get("max_tokens", 500),
        )

        # ç¼“å­˜å®ä¾‹
        cls._instances[llm_type] = llm

        logger.info(
            f"âœ… Created {llm_type} LLM: "
            f"model={model_config.get('model')}, "
            f"temp={model_config.get('temperature')}, "
            f"max_tokens={model_config.get('max_tokens')}"
        )

        return llm

    @classmethod
    def _create_ollama_llm(cls) -> ChatOllama:
        """åˆ›å»º Ollama æœ¬åœ°æ¨¡å‹ï¼ˆç”¨äº Workerï¼‰"""
        try:
            ollama_config = config.get("ollama", {})

            if not ollama_config.get("enabled", False):
                raise ValueError("Ollama is not enabled in config")

            model_name = ollama_config.get("model", "qwen2.5:7b")
            base_url = ollama_config.get("base_url", "http://localhost:11434")
            temperature = ollama_config.get("temperature", 0.0)
            timeout = ollama_config.get("timeout", 60)

            llm = ChatOllama(
                model=model_name,
                base_url=base_url,
                temperature=temperature,
                timeout=timeout,
                # å¯ç”¨å·¥å…·è°ƒç”¨æ”¯æŒ
                num_ctx=4096,  # ä¸Šä¸‹æ–‡é•¿åº¦
            )

            logger.info(
                f"âœ… Created Worker Ollama LLM: "
                f"model={model_name}, "
                f"temp={temperature}, "
                f"base_url={base_url}"
            )

            return llm

        except Exception as e:
            logger.error(f"Failed to create Ollama LLM: {e}")
            logger.warning("âš ï¸  Falling back to Qiniu cloud for Worker")
            # é™çº§åˆ°ä¸ƒç‰›äº‘
            return cls._create_qiniu_llm("worker")

    @classmethod
    def _create_qiniu_llm(cls, llm_type: str) -> ChatOpenAI:
        """åˆ›å»ºä¸ƒç‰›äº‘ LLMï¼ˆç”¨äº Planner å’Œ Summaryï¼‰"""
        try:
            # è·å–åŸºç¡€é…ç½®
            qiniu_config = config.get("qiniu")
            if not qiniu_config:
                raise ValueError("Qiniu config not found")

            # è·å–æ¨¡å‹ç‰¹å®šé…ç½®
            model_config = config.get(f"qiniu.models.{llm_type}")

            if not model_config:
                logger.warning(
                    f"No specific config for {llm_type}, using default config"
                )
                model_config = {
                    "model": "doubao-1.5-pro-32k",
                    "temperature": 0.0,
                    "max_tokens": 500
                }

            # åˆ›å»ºå®ä¾‹
            llm = ChatOpenAI(
                api_key=qiniu_config.get("api_key"),
                base_url=qiniu_config.get("base_url"),
                model=model_config.get("model"),
                temperature=model_config.get("temperature", 0.0),
                max_tokens=model_config.get("max_tokens", 500),
            )

            logger.info(
                f"âœ… Created {llm_type} Qiniu LLM: "
                f"model={model_config.get('model')}, "
                f"temp={model_config.get('temperature')}, "
                f"max_tokens={model_config.get('max_tokens')}"
            )

            return llm

        except Exception as e:
            logger.error(f"Failed to create Qiniu LLM for {llm_type}: {e}")
            raise

    @classmethod
    def get_planner_llm(cls) -> ChatOpenAI:
        """
        è·å– Planner ä¸“ç”¨ LLM

        ä½¿ç”¨ qwen3-maxï¼šæœ€å¼ºæ¨ç†èƒ½åŠ›ï¼Œç”¨äºä»»åŠ¡è§„åˆ’
        """
        return cls.get_llm("planner")

    @classmethod
    def get_worker_llm(cls) -> ChatOpenAI:
        """
        è·å– Worker ä¸“ç”¨ LLM

        ä½¿ç”¨ qwen3-max-previewï¼šå¿«é€Ÿå“åº”ï¼Œç”¨äºå·¥å…·è°ƒç”¨
        """
        return cls.get_llm("worker")

    @classmethod
    def get_summary_llm(cls) -> ChatOpenAI:
        """
        è·å– Summary ä¸“ç”¨ LLM

        ä½¿ç”¨ qwen3-max-previewï¼šå¿«é€Ÿå“åº”ï¼Œç”¨äºç»“æœæ€»ç»“
        """
        return cls.get_llm("summary")

    @classmethod
    def get_model_info(cls, llm_type: str) -> Dict[str, any]:
        """è·å–æ¨¡å‹é…ç½®ä¿¡æ¯"""
        model_config = config.get(f"qiniu.models.{llm_type}")
        return {
            "type": llm_type,
            "model": model_config.get("model"),
            "temperature": model_config.get("temperature"),
            "max_tokens": model_config.get("max_tokens")
        }

    @classmethod
    def clear_cache(cls):
        """æ¸…é™¤ç¼“å­˜ï¼ˆç”¨äºæµ‹è¯•æˆ–é‡æ–°é…ç½®ï¼‰"""
        cls._instances.clear()
        logger.info("ğŸ—‘ï¸  LLM cache cleared")

    @classmethod
    def get_all_models_info(cls) -> Dict[str, Dict]:
        """è·å–æ‰€æœ‰æ¨¡å‹é…ç½®ä¿¡æ¯"""
        return {
            "planner": cls.get_model_info("planner"),
            "worker": cls.get_model_info("worker"),
            "summary": cls.get_model_info("summary")
        }
